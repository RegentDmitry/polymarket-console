#!/usr/bin/env python3
"""
–ú–æ–¥—É–ª—å –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏—Ö –¥–∞–Ω–Ω—ã—Ö earthquake —Ä—ã–Ω–∫–æ–≤.

–°–∫–∞—á–∏–≤–∞–µ—Ç:
1. –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ–±—ã—Ç–∏–π –∏–∑ Gamma API
2. –ò—Å—Ç–æ—Ä–∏—é —Å–¥–µ–ª–æ–∫ –∏–∑ CLOB API
3. –ò—Å—Ç–æ—Ä–∏—é –∑–µ–º–ª–µ—Ç—Ä—è—Å–µ–Ω–∏–π –∏–∑ USGS

–ò—Å–ø–æ–ª—å–∑–æ–≤–∞–Ω–∏–µ:
    python history_downloader.py              # –°–∫–∞—á–∞—Ç—å –≤—Å—ë
    python history_downloader.py --trades     # –¢–æ–ª—å–∫–æ —Å–¥–µ–ª–∫–∏
    python history_downloader.py --usgs       # –¢–æ–ª—å–∫–æ USGS –¥–∞–Ω–Ω—ã–µ
"""

import argparse
import json
import os
import time
from dataclasses import dataclass, asdict
from datetime import datetime, timezone, timedelta
from pathlib import Path
from typing import Optional

import httpx
from dotenv import load_dotenv

load_dotenv()

# –î–∏—Ä–µ–∫—Ç–æ—Ä–∏–∏
BASE_DIR = Path(__file__).parent
HISTORY_DIR = BASE_DIR / "history"
CLOSED_DIR = HISTORY_DIR / "closed"
OPEN_DIR = HISTORY_DIR / "open"
TRADES_DIR = HISTORY_DIR / "trades"
USGS_DIR = HISTORY_DIR / "usgs"

# API URLs
GAMMA_API = "https://gamma-api.polymarket.com"
CLOB_API = os.getenv("CLOB_API_URL", "https://clob.polymarket.com")
USGS_API = "https://earthquake.usgs.gov/fdsnws/event/1"

# CLOB credentials
CLOB_API_KEY = os.getenv("CLOB_API_KEY", "")
CLOB_SECRET = os.getenv("CLOB_SECRET", "")
CLOB_PASS_PHRASE = os.getenv("CLOB_PASS_PHRASE", "")

# Dune Analytics
DUNE_API_KEY = os.getenv("DUNE_API_KEY", "")


# ============================================================================
# DATA CLASSES
# ============================================================================

@dataclass
class Trade:
    """–û–¥–Ω–∞ —Å–¥–µ–ª–∫–∞."""
    timestamp: str
    price: float
    size: float
    side: str  # "BUY" or "SELL"

@dataclass
class PricePoint:
    """–¶–µ–Ω–∞ –Ω–∞ –º–æ–º–µ–Ω—Ç –≤—Ä–µ–º–µ–Ω–∏."""
    timestamp: str
    price: float
    volume_24h: float = 0


# ============================================================================
# GAMMA API - –ú–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ–±—ã—Ç–∏–π
# ============================================================================

# –í—Å–µ earthquake —Å–æ–±—ã—Ç–∏—è
EARTHQUAKE_SLUGS = {
    "closed": [
        "megaquake-in-february",
        "6pt0-earthquake-in-mediterranean-by-next-friday",
        "megaquake-in-january",
        "megaquake-in-december",
        "megaquake-in-november",
        "megaquake-in-october",
        "megaquake-in-september",
        "megaquake-in-august",
        "will-an-earthquake-measuring-80-or-above-occur-anywhere-on-earth-before-june-1-2022",
        "will-there-be-an-earthquake-of-magnitude-4pt5-or-higher-in-the-conterminous-us-by-december-31st",
        "will-there-be-an-earthquake-of-magnitude-4pt5-or-higher-in-conterminous-us-by-november-29",
    ],
    "open": [
        "megaquake-by-january-31",
        "megaquake-by-march-31",
        "megaquake-by-june-30",
        "how-many-7pt0-or-above-earthquakes-by-june-30",
        "how-many-7pt0-or-above-earthquakes-in-2026",
        "9pt0-or-above-earthquake-before-2027",
        "10pt0-or-above-earthquake-before-2027",
    ],
}


def download_event_metadata(slug: str, output_dir: Path) -> Optional[dict]:
    """–°–∫–∞—á–∞—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ —Å–æ–±—ã—Ç–∏—è –∏–∑ Gamma API."""
    try:
        r = httpx.get(f"{GAMMA_API}/events?slug={slug}", timeout=30)
        data = r.json()

        if not data:
            print(f"  ‚ùå {slug}: –Ω–µ –Ω–∞–π–¥–µ–Ω–æ")
            return None

        event = data[0]

        # –°–æ—Ö—Ä–∞–Ω—è–µ–º
        filepath = output_dir / f"{slug}.json"
        with open(filepath, 'w', encoding='utf-8') as f:
            json.dump(event, f, indent=2, ensure_ascii=False)

        title = event.get('title', '')
        volume = event.get('volume', 0)
        print(f"  ‚úÖ {title} (${volume:,.0f})")

        return event

    except Exception as e:
        print(f"  ‚ùå {slug}: {e}")
        return None


def download_all_metadata():
    """–°–∫–∞—á–∞—Ç—å –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ –≤—Å–µ—Ö —Å–æ–±—ã—Ç–∏–π."""
    print("\n" + "=" * 60)
    print("–°–ö–ê–ß–ò–í–ê–ù–ò–ï –ú–ï–¢–ê–î–ê–ù–ù–´–• –°–û–ë–´–¢–ò–ô (Gamma API)")
    print("=" * 60)

    CLOSED_DIR.mkdir(parents=True, exist_ok=True)
    OPEN_DIR.mkdir(parents=True, exist_ok=True)

    all_events = []

    print(f"\nüìÅ –ó–∞–∫—Ä—ã—Ç—ã–µ —Å–æ–±—ã—Ç–∏—è ({len(EARTHQUAKE_SLUGS['closed'])}):\n")
    for slug in EARTHQUAKE_SLUGS['closed']:
        event = download_event_metadata(slug, CLOSED_DIR)
        if event:
            all_events.append(event)
        time.sleep(0.2)  # Rate limiting

    print(f"\nüìÅ –û—Ç–∫—Ä—ã—Ç—ã–µ —Å–æ–±—ã—Ç–∏—è ({len(EARTHQUAKE_SLUGS['open'])}):\n")
    for slug in EARTHQUAKE_SLUGS['open']:
        event = download_event_metadata(slug, OPEN_DIR)
        if event:
            all_events.append(event)
        time.sleep(0.2)

    # –°–≤–æ–¥–∫–∞
    summary = {
        "generated_at": datetime.now(timezone.utc).isoformat(),
        "total_events": len(all_events),
        "total_volume": sum(e.get('volume', 0) for e in all_events),
    }

    with open(HISTORY_DIR / "summary.json", 'w') as f:
        json.dump(summary, f, indent=2)

    print(f"\n‚úÖ –°–∫–∞—á–∞–Ω–æ {len(all_events)} —Å–æ–±—ã—Ç–∏–π")
    return all_events


# ============================================================================
# CLOB API - –ò—Å—Ç–æ—Ä–∏—è —Å–¥–µ–ª–æ–∫
# ============================================================================

def get_clob_headers() -> dict:
    """–ü–æ–ª—É—á–∏—Ç—å –∑–∞–≥–æ–ª–æ–≤–∫–∏ –¥–ª—è CLOB API."""
    if not CLOB_API_KEY:
        return {}
    return {
        "Authorization": f"Bearer {CLOB_API_KEY}",
    }


def download_trades_for_market(condition_id: str, slug: str) -> list[dict]:
    """–°–∫–∞—á–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é —Å–¥–µ–ª–æ–∫ –¥–ª—è –æ–¥–Ω–æ–≥–æ —Ä—ã–Ω–∫–∞."""
    trades = []

    try:
        # –ü—Ä–æ–±—É–µ–º –ø—É–±–ª–∏—á–Ω—ã–π endpoint
        r = httpx.get(
            f"{CLOB_API}/trades",
            params={"market": condition_id, "limit": 500},
            headers=get_clob_headers(),
            timeout=30,
        )

        if r.status_code == 200:
            data = r.json()
            if isinstance(data, list):
                trades = data
            elif isinstance(data, dict) and 'trades' in data:
                trades = data['trades']
        elif r.status_code == 401:
            print(f"    ‚ö†Ô∏è  –¢—Ä–µ–±—É–µ—Ç—Å—è –∞–≤—Ç–æ—Ä–∏–∑–∞—Ü–∏—è –¥–ª—è {slug}")
        else:
            print(f"    ‚ö†Ô∏è  HTTP {r.status_code} –¥–ª—è {slug}")

    except Exception as e:
        print(f"    ‚ùå –û—à–∏–±–∫–∞: {e}")

    return trades




# ============================================================================
# USGS API - –ò—Å—Ç–æ—Ä–∏—è –∑–µ–º–ª–µ—Ç—Ä—è—Å–µ–Ω–∏–π
# ============================================================================

def download_usgs_history(
    start_date: datetime,
    end_date: datetime,
    min_magnitude: float = 4.5,
) -> list[dict]:
    """–°–∫–∞—á–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é –∑–µ–º–ª–µ—Ç—Ä—è—Å–µ–Ω–∏–π —Å USGS."""
    earthquakes = []

    try:
        r = httpx.get(
            f"{USGS_API}/query",
            params={
                "format": "geojson",
                "starttime": start_date.strftime("%Y-%m-%d"),
                "endtime": end_date.strftime("%Y-%m-%d"),
                "minmagnitude": min_magnitude,
                "orderby": "time",
            },
            timeout=60,
        )

        if r.status_code == 200:
            data = r.json()
            for feature in data.get('features', []):
                props = feature.get('properties', {})
                earthquakes.append({
                    "id": feature.get('id'),
                    "time": props.get('time'),
                    "magnitude": props.get('mag'),
                    "place": props.get('place'),
                    "url": props.get('url'),
                })
    except Exception as e:
        print(f"  ‚ùå USGS –æ—à–∏–±–∫–∞: {e}")

    return earthquakes


def download_all_usgs():
    """–°–∫–∞—á–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é –∑–µ–º–ª–µ—Ç—Ä—è—Å–µ–Ω–∏–π –¥–ª—è –≤—Å–µ—Ö —Ä–µ–ª–µ–≤–∞–Ω—Ç–Ω—ã—Ö –ø–µ—Ä–∏–æ–¥–æ–≤."""
    print("\n" + "=" * 60)
    print("–°–ö–ê–ß–ò–í–ê–ù–ò–ï –ò–°–¢–û–†–ò–ò –ó–ï–ú–õ–ï–¢–†–Ø–°–ï–ù–ò–ô (USGS)")
    print("=" * 60)

    USGS_DIR.mkdir(parents=True, exist_ok=True)

    # –ü–µ—Ä–∏–æ–¥—ã –¥–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è (–ø–æ –º–∞–≥–Ω–∏—Ç—É–¥–∞–º)
    periods = [
        # M4.5+ –¥–ª—è US —Ä—ã–Ω–∫–æ–≤ 2021
        {
            "name": "m4.5_us_2021",
            "start": datetime(2021, 10, 1),
            "end": datetime(2022, 1, 1),
            "magnitude": 4.5,
        },
        # M6.0+ –¥–ª—è Mediterranean
        {
            "name": "m6.0_2025",
            "start": datetime(2025, 1, 1),
            "end": datetime(2025, 3, 1),
            "magnitude": 6.0,
        },
        # M7.0+ –≥–ª–æ–±–∞–ª—å–Ω–æ (–¥–ª—è —Ç–µ–∫—É—â–∏—Ö —Ä—ã–Ω–∫–æ–≤)
        {
            "name": "m7.0_global_2024_2026",
            "start": datetime(2024, 1, 1),
            "end": datetime(2026, 12, 31),
            "magnitude": 7.0,
        },
        # M8.0+ –≥–ª–æ–±–∞–ª—å–Ω–æ (megaquake)
        {
            "name": "m8.0_global_2020_2026",
            "start": datetime(2020, 1, 1),
            "end": datetime(2026, 12, 31),
            "magnitude": 8.0,
        },
        # M9.0+ –≥–ª–æ–±–∞–ª—å–Ω–æ
        {
            "name": "m9.0_global_2000_2026",
            "start": datetime(2000, 1, 1),
            "end": datetime(2026, 12, 31),
            "magnitude": 9.0,
        },
    ]

    total_quakes = 0

    for period in periods:
        print(f"\nüìä {period['name']} (M{period['magnitude']}+)...")

        quakes = download_usgs_history(
            period['start'],
            period['end'],
            period['magnitude'],
        )

        if quakes:
            filepath = USGS_DIR / f"{period['name']}.json"
            with open(filepath, 'w') as f:
                json.dump({
                    "period": period['name'],
                    "start_date": period['start'].isoformat(),
                    "end_date": period['end'].isoformat(),
                    "min_magnitude": period['magnitude'],
                    "count": len(quakes),
                    "earthquakes": quakes,
                }, f, indent=2)

            print(f"  ‚úÖ {len(quakes)} –∑–µ–º–ª–µ—Ç—Ä—è—Å–µ–Ω–∏–π")
            total_quakes += len(quakes)
        else:
            print(f"  ‚ö†Ô∏è  0 –∑–µ–º–ª–µ—Ç—Ä—è—Å–µ–Ω–∏–π")

        time.sleep(0.5)

    print(f"\n‚úÖ –í—Å–µ–≥–æ —Å–∫–∞—á–∞–Ω–æ {total_quakes} –∑–∞–ø–∏—Å–µ–π –æ –∑–µ–º–ª–µ—Ç—Ä—è—Å–µ–Ω–∏—è—Ö")
    return total_quakes


# ============================================================================
# DUNE ANALYTICS - –ò—Å—Ç–æ—Ä–∏—è —Å–¥–µ–ª–æ–∫
# ============================================================================

DUNE_API = "https://api.dune.com/api/v1"

# –ì–æ—Ç–æ–≤—ã–µ –∑–∞–ø—Ä–æ—Å—ã –¥–ª—è earthquake markets
# –ú–æ–∂–Ω–æ —Å–æ–∑–¥–∞—Ç—å —Å–≤–æ–π –∑–∞–ø—Ä–æ—Å –Ω–∞ dune.com –∏ –∏—Å–ø–æ–ª—å–∑–æ–≤–∞—Ç—å –µ–≥–æ ID
DUNE_QUERIES = {
    # –ü—Ä–∏–º–µ—Ä: –∏—Å—Ç–æ—Ä–∏—è —Å–¥–µ–ª–æ–∫ –ø–æ –≤—Å–µ–º polymarket —Ä—ã–Ω–∫–∞–º
    "polymarket_trades": 3145285,  # –ó–∞–º–µ–Ω–∏ –Ω–∞ –∞–∫—Ç—É–∞–ª—å–Ω—ã–π query_id
}


def run_dune_query(query_id: int, params: dict = None) -> list[dict]:
    """–í—ã–ø–æ–ª–Ω–∏—Ç—å –∑–∞–ø—Ä–æ—Å Dune Analytics."""
    if not DUNE_API_KEY:
        print("  ‚ö†Ô∏è  DUNE_API_KEY –Ω–µ —É—Å—Ç–∞–Ω–æ–≤–ª–µ–Ω –≤ .env")
        print("  ‚Üí –ü–æ–ª—É—á–∏ –±–µ—Å–ø–ª–∞—Ç–Ω—ã–π –∫–ª—é—á: https://dune.com/settings/api")
        return []

    headers = {"X-Dune-API-Key": DUNE_API_KEY}

    try:
        # –ó–∞–ø—É—Å–∫–∞–µ–º –≤—ã–ø–æ–ª–Ω–µ–Ω–∏–µ –∑–∞–ø—Ä–æ—Å–∞
        print(f"  –ó–∞–ø—É—Å–∫–∞–µ–º Dune query {query_id}...")
        r = httpx.post(
            f"{DUNE_API}/query/{query_id}/execute",
            headers=headers,
            json={"query_parameters": params or {}},
            timeout=30,
        )

        if r.status_code != 200:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –∑–∞–ø—É—Å–∫–∞: {r.status_code} - {r.text[:200]}")
            return []

        execution_id = r.json().get("execution_id")
        if not execution_id:
            print("  ‚ùå –ù–µ—Ç execution_id")
            return []

        # –ñ–¥—ë–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç
        print(f"  –û–∂–∏–¥–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç (execution_id: {execution_id})...")
        for _ in range(60):  # –ú–∞–∫—Å–∏–º—É–º 5 –º–∏–Ω—É—Ç
            time.sleep(5)

            r = httpx.get(
                f"{DUNE_API}/execution/{execution_id}/status",
                headers=headers,
                timeout=30,
            )
            status = r.json().get("state")
            print(f"    Status: {status}")

            if status == "QUERY_STATE_COMPLETED":
                break
            elif status in ["QUERY_STATE_FAILED", "QUERY_STATE_CANCELLED"]:
                print(f"  ‚ùå Query failed: {status}")
                return []

        # –ü–æ–ª—É—á–∞–µ–º —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã
        r = httpx.get(
            f"{DUNE_API}/execution/{execution_id}/results",
            headers=headers,
            timeout=60,
        )

        if r.status_code == 200:
            data = r.json()
            rows = data.get("result", {}).get("rows", [])
            print(f"  ‚úÖ –ü–æ–ª—É—á–µ–Ω–æ {len(rows)} –∑–∞–ø–∏—Å–µ–π")
            return rows
        else:
            print(f"  ‚ùå –û—à–∏–±–∫–∞ –ø–æ–ª—É—á–µ–Ω–∏—è —Ä–µ–∑—É–ª—å—Ç–∞—Ç–æ–≤: {r.status_code}")
            return []

    except Exception as e:
        print(f"  ‚ùå Dune error: {e}")
        return []


def create_earthquake_trades_query() -> str:
    """SQL –∑–∞–ø—Ä–æ—Å –¥–ª—è –ø–æ–ª—É—á–µ–Ω–∏—è —Å–¥–µ–ª–æ–∫ –ø–æ earthquake —Ä—ã–Ω–∫–∞–º."""
    return """
    SELECT
        block_time,
        tx_hash,
        trader,
        side,
        size,
        price,
        outcome,
        market_slug
    FROM polymarket.trades
    WHERE (
        LOWER(market_slug) LIKE '%earthquake%'
        OR LOWER(market_slug) LIKE '%megaquake%'
        OR LOWER(market_slug) LIKE '%9pt0%'
        OR LOWER(market_slug) LIKE '%10pt0%'
        OR LOWER(market_slug) LIKE '%7pt0%'
    )
    ORDER BY block_time DESC
    """


def download_dune_trades():
    """–°–∫–∞—á–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—é —Å–¥–µ–ª–æ–∫ —á–µ—Ä–µ–∑ Dune Analytics."""
    print("\n" + "=" * 60)
    print("–°–ö–ê–ß–ò–í–ê–ù–ò–ï –ò–°–¢–û–†–ò–ò –°–î–ï–õ–û–ö (Dune Analytics)")
    print("=" * 60)

    if not DUNE_API_KEY:
        print("\n‚ö†Ô∏è  –î–ª—è —Å–∫–∞—á–∏–≤–∞–Ω–∏—è –∏—Å—Ç–æ—Ä–∏–∏ —Å–¥–µ–ª–æ–∫ –Ω—É–∂–µ–Ω Dune API key")
        print("\n–ö–∞–∫ –ø–æ–ª—É—á–∏—Ç—å (–±–µ—Å–ø–ª–∞—Ç–Ω–æ):")
        print("  1. –ó–∞—Ä–µ–≥–∏—Å—Ç—Ä–∏—Ä—É–π—Å—è –Ω–∞ https://dune.com")
        print("  2. –ü–µ—Ä–µ–π–¥–∏ –≤ Settings ‚Üí API")
        print("  3. –°–æ–∑–¥–∞–π API key")
        print("  4. –î–æ–±–∞–≤—å –≤ .env: DUNE_API_KEY=—Ç–≤–æ–π_–∫–ª—é—á")
        print("\n–ê–ª—å—Ç–µ—Ä–Ω–∞—Ç–∏–≤–∞ - —Ä—É—á–Ω–æ–π —ç–∫—Å–ø–æ—Ä—Ç CSV:")
        print("  ‚Üí https://dune.com/polymarket")
        print("  ‚Üí –ù–∞–π–¥–∏ –Ω—É–∂–Ω—ã–π –∑–∞–ø—Ä–æ—Å ‚Üí Export ‚Üí CSV")
        return []

    TRADES_DIR.mkdir(parents=True, exist_ok=True)

    # –ò—Å–ø–æ–ª—å–∑—É–µ–º –≥–æ—Ç–æ–≤—ã–π –∑–∞–ø—Ä–æ—Å –∏–ª–∏ —Å–æ–∑–¥–∞—ë–º —Å–≤–æ–π
    # –ü—Ä–∏–º–µ—á–∞–Ω–∏–µ: –¥–ª—è —Å–≤–æ–µ–≥–æ –∑–∞–ø—Ä–æ—Å–∞ –Ω—É–∂–Ω–æ —Å–Ω–∞—á–∞–ª–∞ —Å–æ–∑–¥–∞—Ç—å –µ–≥–æ –Ω–∞ dune.com

    print("\nüìä –ó–∞–ø—Ä–∞—à–∏–≤–∞–µ–º –¥–∞–Ω–Ω—ã–µ...")

    # –ü–æ–ø—Ä–æ–±—É–µ–º –ø–æ–ª—É—á–∏—Ç—å —Ä–µ–∑—É–ª—å—Ç–∞—Ç—ã –ø–æ—Å–ª–µ–¥–Ω–µ–≥–æ –≤—ã–ø–æ–ª–Ω–µ–Ω–∏—è (–±—ã—Å—Ç—Ä–µ–µ)
    query_id = DUNE_QUERIES.get("polymarket_trades")

    try:
        headers = {"X-Dune-API-Key": DUNE_API_KEY}
        r = httpx.get(
            f"{DUNE_API}/query/{query_id}/results",
            headers=headers,
            timeout=60,
        )

        if r.status_code == 200:
            data = r.json()
            rows = data.get("result", {}).get("rows", [])

            # –§–∏–ª—å—Ç—Ä—É–µ–º —Ç–æ–ª—å–∫–æ earthquake
            earthquake_trades = [
                row for row in rows
                if any(kw in str(row.get('market_slug', '')).lower()
                       for kw in ['earthquake', 'megaquake', '7pt0', '8pt0', '9pt0', '10pt0'])
            ]

            if earthquake_trades:
                filepath = TRADES_DIR / "dune_earthquake_trades.json"
                with open(filepath, 'w') as f:
                    json.dump({
                        "source": "dune_analytics",
                        "query_id": query_id,
                        "downloaded_at": datetime.now(timezone.utc).isoformat(),
                        "count": len(earthquake_trades),
                        "trades": earthquake_trades,
                    }, f, indent=2)

                print(f"  ‚úÖ –°–æ—Ö—Ä–∞–Ω–µ–Ω–æ {len(earthquake_trades)} —Å–¥–µ–ª–æ–∫")
                return earthquake_trades
            else:
                print("  ‚ö†Ô∏è  –ù–µ—Ç earthquake —Å–¥–µ–ª–æ–∫ –≤ —Ä–µ–∑—É–ª—å—Ç–∞—Ç–∞—Ö")
        else:
            print(f"  ‚ö†Ô∏è  HTTP {r.status_code}: {r.text[:200]}")

    except Exception as e:
        print(f"  ‚ùå –û—à–∏–±–∫–∞: {e}")

    return []


# ============================================================================
# MAIN
# ============================================================================

def main():
    parser = argparse.ArgumentParser(description="–°–∫–∞—á–∞—Ç—å –∏—Å—Ç–æ—Ä–∏—á–µ—Å–∫–∏–µ –¥–∞–Ω–Ω—ã–µ")
    parser.add_argument("--metadata", action="store_true", help="–¢–æ–ª—å–∫–æ –º–µ—Ç–∞–¥–∞–Ω–Ω—ã–µ")
    parser.add_argument("--trades", action="store_true", help="–¢–æ–ª—å–∫–æ —Å–¥–µ–ª–∫–∏ (Dune)")
    parser.add_argument("--usgs", action="store_true", help="–¢–æ–ª—å–∫–æ USGS")
    parser.add_argument("--dune", action="store_true", help="–¢–æ–ª—å–∫–æ Dune Analytics")
    args = parser.parse_args()

    # –ï—Å–ª–∏ –Ω–∏—á–µ–≥–æ –Ω–µ —É–∫–∞–∑–∞–Ω–æ - —Å–∫–∞—á–∏–≤–∞–µ–º –≤—Å—ë
    download_all = not (args.metadata or args.trades or args.usgs or args.dune)

    print("=" * 60)
    print("EARTHQUAKE HISTORY DOWNLOADER")
    print("=" * 60)
    print(f"–í—Ä–µ–º—è: {datetime.now(timezone.utc).strftime('%Y-%m-%d %H:%M UTC')}")
    print(f"–î–∏—Ä–µ–∫—Ç–æ—Ä–∏—è: {HISTORY_DIR}")

    if download_all or args.metadata:
        download_all_metadata()

    if download_all or args.trades or args.dune:
        download_dune_trades()

    if download_all or args.usgs:
        download_all_usgs()

    print("\n" + "=" * 60)
    print("–ì–û–¢–û–í–û!")
    print("=" * 60)

    # –ü–æ–∫–∞–∑—ã–≤–∞–µ–º —á—Ç–æ —Å–∫–∞—á–∞–Ω–æ
    print("\n–°–æ–¥–µ—Ä–∂–∏–º–æ–µ history/:")
    for item in sorted(HISTORY_DIR.rglob("*.json")):
        rel_path = item.relative_to(HISTORY_DIR)
        size = item.stat().st_size / 1024
        print(f"  {rel_path} ({size:.1f} KB)")

    # –ò–Ω—Å—Ç—Ä—É–∫—Ü–∏–∏ –µ—Å–ª–∏ –Ω–µ—Ç Dune key
    if not DUNE_API_KEY:
        print("\n" + "-" * 60)
        print("üí° –î–ª—è –ø–æ–ª–Ω–æ–π –∏—Å—Ç–æ—Ä–∏–∏ —Å–¥–µ–ª–æ–∫ –¥–æ–±–∞–≤—å –≤ .env:")
        print("   DUNE_API_KEY=—Ç–≤–æ–π_–∫–ª—é—á")
        print("   ‚Üí https://dune.com/settings/api (–±–µ—Å–ø–ª–∞—Ç–Ω–æ)")
        print("-" * 60)


if __name__ == "__main__":
    main()
